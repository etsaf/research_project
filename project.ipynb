{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='up'></a>\n",
    "\n",
    "## Содержание jupyter notebook (ссылки):\n",
    "1. Подключение библиотек и функций для получения вейвлет-спектра\n",
    "2. [Генерация 40000 синтетических сигналов](#gen)\n",
    "3. [Получение информации об областях значимости](#info)\n",
    "4. Нейронные сети:\n",
    "&emsp;[Перцептрон](#network1)\n",
    "&emsp;[Нейронная сеть с двумя входами](#network2)\n",
    "&emsp;[Сверточная нейронная сеть](#network3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import emd\n",
    "\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В поле ниже длинный код функции для вейвлет-преобразования, взятой с GitHub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 1995-2021, Christopher Torrence and Gilbert P.Compo\n",
    "# Python version of the code is written by Evgeniya Predybaylo in 2014\n",
    "# edited by Michael von Papen (FZ Juelich, INM-6), 2018, to include\n",
    "# analysis at arbitrary frequencies\n",
    "#\n",
    "#   This software may be used, copied, or redistributed as long as it is not\n",
    "#   sold and this copyright notice is reproduced on each copy made. This\n",
    "#   routine is provided as is without any express or implied warranties\n",
    "#   whatsoever.\n",
    "#\n",
    "# Notice: Please acknowledge the use of the above software in any publications:\n",
    "#            Wavelet software was provided by C. Torrence and G. Compo,\n",
    "#      and is available at URL: http://paos.colorado.edu/research/wavelets/''.\n",
    "#\n",
    "# Reference: Torrence, C. and G. P. Compo, 1998: A Practical Guide to\n",
    "#            Wavelet Analysis. <I>Bull. Amer. Meteor. Soc.</I>, 79, 61-78.\n",
    "#\n",
    "# Please send a copy of such publications to either C. Torrence or G. Compo:\n",
    "#  Dr. Christopher Torrence               Dr. Gilbert P. Compo\n",
    "#  Research Systems, Inc.                 Climate Diagnostics Center\n",
    "#  4990 Pearl East Circle                 325 Broadway R/CDC1\n",
    "#  Boulder, CO 80301, USA                 Boulder, CO 80305-3328, USA\n",
    "#  E-mail: chris[AT]rsinc[DOT]com         E-mail: compo[AT]colorado[DOT]edu\n",
    "#\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# # WAVELET  1D Wavelet transform with optional significance testing\n",
    "#   wave, period, scale, coi = wavelet(Y, dt, pad, dj, s0, J1, mother, param)\n",
    "#\n",
    "#   Computes the wavelet transform of the vector Y (length N),\n",
    "#   with sampling rate DT.\n",
    "#\n",
    "#   By default, the Morlet wavelet (k0=6) is used.\n",
    "#   The wavelet basis is normalized to have total energy=1 at all scales.\n",
    "#\n",
    "# INPUTS:\n",
    "#\n",
    "#    Y = the time series of length N.\n",
    "#    DT = amount of time between each Y value, i.e. the sampling time.\n",
    "#\n",
    "# OUTPUTS:\n",
    "#\n",
    "#    WAVE is the WAVELET transform of Y. This is a complex array\n",
    "#    of dimensions (N,J1+1). FLOAT(WAVE) gives the WAVELET amplitude,\n",
    "#    ATAN(IMAGINARY(WAVE),FLOAT(WAVE) gives the WAVELET phase.\n",
    "#    The WAVELET power spectrum is ABS(WAVE)**2.\n",
    "#    Its units are sigma**2 (the time series variance).\n",
    "#\n",
    "# OPTIONAL INPUTS:\n",
    "#\n",
    "# *** Note *** if none of the optional variables is set up, then the program\n",
    "#   uses default values of -1.\n",
    "#\n",
    "#    PAD = if set to 1 (default is 0), pad time series with zeroes to get\n",
    "#         N up to the next higher power of 2. This prevents wraparound\n",
    "#         from the end of the time series to the beginning, and also\n",
    "#         speeds up the FFT's used to do the wavelet transform.\n",
    "#         This will not eliminate all edge effects (see COI below).\n",
    "#\n",
    "#    DJ = the spacing between discrete scales. Default is 0.25.\n",
    "#         A smaller # will give better scale resolution, but be slower to plot.\n",
    "#\n",
    "#    S0 = the smallest scale of the wavelet.  Default is 2*DT.\n",
    "#\n",
    "#    J1 = the # of scales minus one. Scales range from S0 up to S0*2**(J1*DJ),\n",
    "#        to give a total of (J1+1) scales. Default is J1 = (LOG2(N DT/S0))/DJ.\n",
    "#\n",
    "#    MOTHER = the mother wavelet function.\n",
    "#             The choices are 'MORLET', 'PAUL', or 'DOG'\n",
    "#\n",
    "#    PARAM = the mother wavelet parameter.\n",
    "#            For 'MORLET' this is k0 (wavenumber), default is 6.\n",
    "#            For 'PAUL' this is m (order), default is 4.\n",
    "#            For 'DOG' this is m (m-th derivative), default is 2.\n",
    "#\n",
    "#\n",
    "# OPTIONAL OUTPUTS:\n",
    "#\n",
    "#    PERIOD = the vector of \"Fourier\" periods (in time units) that corresponds\n",
    "#           to the SCALEs.\n",
    "#\n",
    "#    SCALE = the vector of scale indices, given by S0*2**(j*DJ), j=0...J1\n",
    "#            where J1+1 is the total # of scales.\n",
    "#\n",
    "#    COI = if specified, then return the Cone-of-Influence, which is a vector\n",
    "#        of N points that contains the maximum period of useful information\n",
    "#        at that particular time.\n",
    "#        Periods greater than this are subject to edge effects.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import fminbound\n",
    "from scipy.special._ufuncs import gamma, gammainc\n",
    "\n",
    "__author__ = 'Evgeniya Predybaylo, Michael von Papen'\n",
    "\n",
    "\n",
    "def wavelet(Y, dt, pad=0, dj=-1, s0=-1, J1=-1, mother=-1, param=-1, freq=None):\n",
    "    n1 = len(Y)\n",
    "\n",
    "    if s0 == -1:\n",
    "        s0 = 2 * dt\n",
    "    if dj == -1:\n",
    "        dj = 1. / 4.\n",
    "    if J1 == -1:\n",
    "        J1 = np.fix((np.log(n1 * dt / s0) / np.log(2)) / dj)\n",
    "    if mother == -1:\n",
    "        mother = 'MORLET'\n",
    "\n",
    "    # construct time series to analyze, pad if necessary\n",
    "    x = Y - np.mean(Y)\n",
    "    if pad == 1:\n",
    "        # power of 2 nearest to N\n",
    "        base2 = np.fix(np.log(n1) / np.log(2) + 0.4999)\n",
    "        nzeroes = (2 ** (base2 + 1) - n1).astype(np.int64)\n",
    "        x = np.concatenate((x, np.zeros(nzeroes)))\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    # construct wavenumber array used in transform [Eqn(5)]\n",
    "    kplus = np.arange(1, int(n / 2) + 1)\n",
    "    kplus = (kplus * 2 * np.pi / (n * dt))\n",
    "    kminus = np.arange(1, int((n - 1) / 2) + 1)\n",
    "    kminus = np.sort((-kminus * 2 * np.pi / (n * dt)))\n",
    "    k = np.concatenate(([0.], kplus, kminus))\n",
    "\n",
    "    # compute FFT of the (padded) time series\n",
    "    f = np.fft.fft(x)  # [Eqn(3)]\n",
    "\n",
    "    # construct SCALE array & empty PERIOD & WAVE arrays\n",
    "    if mother.upper() == 'MORLET':\n",
    "        if param == -1:\n",
    "            param = 6.\n",
    "        fourier_factor = 4 * np.pi / (param + np.sqrt(2 + param**2))\n",
    "    elif mother.upper() == 'PAUL':\n",
    "        if param == -1:\n",
    "            param = 4.\n",
    "        fourier_factor = 4 * np.pi / (2 * param + 1)\n",
    "    elif mother.upper() == 'DOG':\n",
    "        if param == -1:\n",
    "            param = 2.\n",
    "        fourier_factor = 2 * np.pi * np.sqrt(2. / (2 * param + 1))\n",
    "    else:\n",
    "        fourier_factor = np.nan\n",
    "\n",
    "    if freq is None:\n",
    "        j = np.arange(0, J1 + 1)\n",
    "        scale = s0 * 2. ** (j * dj)\n",
    "        freq = 1. / (fourier_factor * scale)\n",
    "        period = 1. / freq\n",
    "    else:\n",
    "        scale = 1. / (fourier_factor * freq)\n",
    "        period = 1. / freq\n",
    "    # define the wavelet array\n",
    "    wave = np.zeros(shape=(len(scale), n), dtype=complex)\n",
    "\n",
    "    # loop through all scales and compute transform\n",
    "    for a1 in range(0, len(scale)):\n",
    "        daughter, fourier_factor, coi, _ = \\\n",
    "            wave_bases(mother, k, scale[a1], param)\n",
    "        wave[a1, :] = np.fft.ifft(f * daughter)  # wavelet transform[Eqn(4)]\n",
    "\n",
    "    # COI [Sec.3g]\n",
    "    coi = coi * dt * np.concatenate((\n",
    "        np.insert(np.arange(int((n1 + 1) / 2) - 1), [0], [1E-5]),\n",
    "        np.insert(np.flipud(np.arange(0, int(n1 / 2) - 1)), [-1], [1E-5])))\n",
    "    wave = wave[:, :n1]  # get rid of padding before returning\n",
    "\n",
    "    return wave, period, scale, coi\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# WAVE_BASES  1D Wavelet functions Morlet, Paul, or DOG\n",
    "#\n",
    "#  DAUGHTER,FOURIER_FACTOR,COI,DOFMIN = wave_bases(MOTHER,K,SCALE,PARAM)\n",
    "#\n",
    "#   Computes the wavelet function as a function of Fourier frequency,\n",
    "#   used for the wavelet transform in Fourier space.\n",
    "#   (This program is called automatically by WAVELET)\n",
    "#\n",
    "# INPUTS:\n",
    "#\n",
    "#    MOTHER = a string, equal to 'MORLET' or 'PAUL' or 'DOG'\n",
    "#    K = a vector, the Fourier frequencies at which to calculate the wavelet\n",
    "#    SCALE = a number, the wavelet scale\n",
    "#    PARAM = the nondimensional parameter for the wavelet function\n",
    "#\n",
    "# OUTPUTS:\n",
    "#\n",
    "#    DAUGHTER = a vector, the wavelet function\n",
    "#    FOURIER_FACTOR = the ratio of Fourier period to scale\n",
    "#    COI = a number, the cone-of-influence size at the scale\n",
    "#    DOFMIN = a number, degrees of freedom for each point in the wavelet power\n",
    "#             (either 2 for Morlet and Paul, or 1 for the DOG)\n",
    "\n",
    "def wave_bases(mother, k, scale, param):\n",
    "    n = len(k)\n",
    "    kplus = np.array(k > 0., dtype=float)\n",
    "\n",
    "    if mother == 'MORLET':  # -----------------------------------  Morlet\n",
    "\n",
    "        if param == -1:\n",
    "            param = 6.\n",
    "\n",
    "        k0 = np.copy(param)\n",
    "        # calc psi_0(s omega) from Table 1\n",
    "        expnt = -(scale * k - k0) ** 2 / 2. * kplus\n",
    "        norm = np.sqrt(scale * k[1]) * (np.pi ** (-0.25)) * np.sqrt(n)\n",
    "        daughter = norm * np.exp(expnt)\n",
    "        daughter = daughter * kplus  # Heaviside step function\n",
    "        # Scale-->Fourier [Sec.3h]\n",
    "        fourier_factor = (4 * np.pi) / (k0 + np.sqrt(2 + k0 ** 2))\n",
    "        coi = fourier_factor / np.sqrt(2)  # Cone-of-influence [Sec.3g]\n",
    "        dofmin = 2  # Degrees of freedom\n",
    "    elif mother == 'PAUL':  # --------------------------------  Paul\n",
    "        if param == -1:\n",
    "            param = 4.\n",
    "        m = param\n",
    "        # calc psi_0(s omega) from Table 1\n",
    "        expnt = -scale * k * kplus\n",
    "        norm_bottom = np.sqrt(m * np.prod(np.arange(1, (2 * m))))\n",
    "        norm = np.sqrt(scale * k[1]) * (2 ** m / norm_bottom) * np.sqrt(n)\n",
    "        daughter = norm * ((scale * k) ** m) * np.exp(expnt) * kplus\n",
    "        fourier_factor = 4 * np.pi / (2 * m + 1)\n",
    "        coi = fourier_factor * np.sqrt(2)\n",
    "        dofmin = 2\n",
    "    elif mother == 'DOG':  # --------------------------------  DOG\n",
    "        if param == -1:\n",
    "            param = 2.\n",
    "        m = param\n",
    "        # calc psi_0(s omega) from Table 1\n",
    "        expnt = -(scale * k) ** 2 / 2.0\n",
    "        norm = np.sqrt(scale * k[1] / gamma(m + 0.5)) * np.sqrt(n)\n",
    "        daughter = -norm * (1j ** m) * ((scale * k) ** m) * np.exp(expnt)\n",
    "        fourier_factor = 2 * np.pi * np.sqrt(2. / (2 * m + 1))\n",
    "        coi = fourier_factor / np.sqrt(2)\n",
    "        dofmin = 1\n",
    "    else:\n",
    "        print('Mother must be one of MORLET, PAUL, DOG')\n",
    "\n",
    "    return daughter, fourier_factor, coi, dofmin\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# WAVE_SIGNIF  Significance testing for the 1D Wavelet transform WAVELET\n",
    "#\n",
    "#   SIGNIF = wave_signif(Y,DT,SCALE,SIGTEST,LAG1,SIGLVL,DOF,MOTHER,PARAM)\n",
    "#\n",
    "# INPUTS:\n",
    "#\n",
    "#    Y = the time series, or, the VARIANCE of the time series.\n",
    "#        (If this is a single number, it is assumed to be the variance...)\n",
    "#    DT = amount of time between each Y value, i.e. the sampling time.\n",
    "#    SCALE = the vector of scale indices, from previous call to WAVELET.\n",
    "#\n",
    "#\n",
    "# OUTPUTS:\n",
    "#\n",
    "#    SIGNIF = significance levels as a function of SCALE\n",
    "#    FFT_THEOR = output theoretical red-noise spectrum as fn of PERIOD\n",
    "#\n",
    "#\n",
    "# OPTIONAL INPUTS:\n",
    "#    SIGTEST = 0, 1, or 2.    If omitted, then assume 0.\n",
    "#\n",
    "#         If 0 (the default), then just do a regular chi-square test,\n",
    "#             i.e. Eqn (18) from Torrence & Compo.\n",
    "#         If 1, then do a \"time-average\" test, i.e. Eqn (23).\n",
    "#             In this case, DOF should be set to NA, the number\n",
    "#             of local wavelet spectra that were averaged together.\n",
    "#             For the Global Wavelet Spectrum, this would be NA=N,\n",
    "#             where N is the number of points in your time series.\n",
    "#         If 2, then do a \"scale-average\" test, i.e. Eqns (25)-(28).\n",
    "#             In this case, DOF should be set to a\n",
    "#             two-element vector [S1,S2], which gives the scale\n",
    "#             range that was averaged together.\n",
    "#             e.g. if one scale-averaged scales between 2 and 8,\n",
    "#             then DOF=[2,8].\n",
    "#\n",
    "#    LAG1 = LAG 1 Autocorrelation, used for SIGNIF levels. Default is 0.0\n",
    "#\n",
    "#    SIGLVL = significance level to use. Default is 0.95\n",
    "#\n",
    "#    DOF = degrees-of-freedom for signif test.\n",
    "#         IF SIGTEST=0, then (automatically) DOF = 2 (or 1 for MOTHER='DOG')\n",
    "#         IF SIGTEST=1, then DOF = NA, the number of times averaged together.\n",
    "#         IF SIGTEST=2, then DOF = [S1,S2], the range of scales averaged.\n",
    "#\n",
    "#       Note: IF SIGTEST=1, then DOF can be a vector (same length as SCALEs),\n",
    "#            in which case NA is assumed to vary with SCALE.\n",
    "#            This allows one to average different numbers of times\n",
    "#            together at different scales, or to take into account\n",
    "#            things like the Cone of Influence.\n",
    "#            See discussion following Eqn (23) in Torrence & Compo.\n",
    "#\n",
    "#    GWS = global wavelet spectrum, a vector of the same length as scale.\n",
    "#          If input then this is used as the theoretical background spectrum,\n",
    "#          rather than white or red noise.\n",
    "\n",
    "def wave_signif(Y, dt, scale, sigtest=0, lag1=0.0, siglvl=0.95,\n",
    "                dof=None, mother='MORLET', param=None, gws=None):\n",
    "    n1 = len(np.atleast_1d(Y))\n",
    "    J1 = len(scale) - 1\n",
    "    dj = np.log2(scale[1] / scale[0])\n",
    "\n",
    "    if n1 == 1:\n",
    "        variance = Y\n",
    "    else:\n",
    "        variance = np.std(Y) ** 2\n",
    "\n",
    "    # get the appropriate parameters [see Table(2)]\n",
    "    if mother == 'MORLET':  # ----------------------------------  Morlet\n",
    "        empir = ([2., -1, -1, -1])\n",
    "        if param is None:\n",
    "            param = 6.\n",
    "            empir[1:] = ([0.776, 2.32, 0.60])\n",
    "        k0 = param\n",
    "        # Scale-->Fourier [Sec.3h]\n",
    "        fourier_factor = (4 * np.pi) / (k0 + np.sqrt(2 + k0 ** 2))\n",
    "    elif mother == 'PAUL':\n",
    "        empir = ([2, -1, -1, -1])\n",
    "        if param is None:\n",
    "            param = 4\n",
    "            empir[1:] = ([1.132, 1.17, 1.5])\n",
    "        m = param\n",
    "        fourier_factor = (4 * np.pi) / (2 * m + 1)\n",
    "    elif mother == 'DOG':  # -------------------------------------Paul\n",
    "        empir = ([1., -1, -1, -1])\n",
    "        if param is None:\n",
    "            param = 2.\n",
    "            empir[1:] = ([3.541, 1.43, 1.4])\n",
    "        elif param == 6:  # --------------------------------------DOG\n",
    "            empir[1:] = ([1.966, 1.37, 0.97])\n",
    "        m = param\n",
    "        fourier_factor = 2 * np.pi * np.sqrt(2. / (2 * m + 1))\n",
    "    else:\n",
    "        print('Mother must be one of MORLET, PAUL, DOG')\n",
    "\n",
    "    period = scale * fourier_factor\n",
    "    dofmin = empir[0]  # Degrees of freedom with no smoothing\n",
    "    Cdelta = empir[1]  # reconstruction factor\n",
    "    gamma_fac = empir[2]  # time-decorrelation factor\n",
    "    dj0 = empir[3]  # scale-decorrelation factor\n",
    "\n",
    "    freq = dt / period  # normalized frequency\n",
    "\n",
    "    if gws is not None:   # use global-wavelet as background spectrum\n",
    "        fft_theor = gws\n",
    "    else:\n",
    "        # [Eqn(16)]\n",
    "        fft_theor = (1 - lag1 ** 2) / \\\n",
    "            (1 - 2 * lag1 * np.cos(freq * 2 * np.pi) + lag1 ** 2)\n",
    "        fft_theor = variance * fft_theor  # include time-series variance\n",
    "\n",
    "    signif = fft_theor\n",
    "    if dof is None:\n",
    "        dof = dofmin\n",
    "\n",
    "    if sigtest == 0:  # no smoothing, DOF=dofmin [Sec.4]\n",
    "        dof = dofmin\n",
    "        chisquare = chisquare_inv(siglvl, dof) / dof\n",
    "        signif = fft_theor * chisquare  # [Eqn(18)]\n",
    "    elif sigtest == 1:  # time-averaged significance\n",
    "        if len(np.atleast_1d(dof)) == 1:\n",
    "            dof = np.zeros(J1) + dof\n",
    "        dof[dof < 1] = 1\n",
    "        # [Eqn(23)]\n",
    "        dof = dofmin * np.sqrt(1 + (dof * dt / gamma_fac / scale) ** 2)\n",
    "        dof[dof < dofmin] = dofmin   # minimum DOF is dofmin\n",
    "        for a1 in range(0, J1 + 1):\n",
    "            chisquare = chisquare_inv(siglvl, dof[a1]) / dof[a1]\n",
    "            signif[a1] = fft_theor[a1] * chisquare\n",
    "    elif sigtest == 2:  # time-averaged significance\n",
    "        if len(dof) != 2:\n",
    "            print('ERROR: DOF must be set to [S1,S2],'\n",
    "                ' the range of scale-averages')\n",
    "        if Cdelta == -1:\n",
    "            print('ERROR: Cdelta & dj0 not defined'\n",
    "                  ' for ' + mother + ' with param = ' + str(param))\n",
    "\n",
    "        s1 = dof[0]\n",
    "        s2 = dof[1]\n",
    "        avg = np.logical_and(scale >= 2, scale < 8)  # scales between S1 & S2\n",
    "        navg = np.sum(np.array(np.logical_and(scale >= 2, scale < 8),\n",
    "            dtype=int))\n",
    "        if navg == 0:\n",
    "            print('ERROR: No valid scales between ' + s1 + ' and ' + s2)\n",
    "        Savg = 1. / np.sum(1. / scale[avg])  # [Eqn(25)]\n",
    "        Smid = np.exp((np.log(s1) + np.log(s2)) / 2.)  # power-of-two midpoint\n",
    "        dof = (dofmin * navg * Savg / Smid) * \\\n",
    "            np.sqrt(1 + (navg * dj / dj0) ** 2)  # [Eqn(28)]\n",
    "        fft_theor = Savg * np.sum(fft_theor[avg] / scale[avg])  # [Eqn(27)]\n",
    "        chisquare = chisquare_inv(siglvl, dof) / dof\n",
    "        signif = (dj * dt / Cdelta / Savg) * fft_theor * chisquare  # [Eqn(26)]\n",
    "    else:\n",
    "        print('ERROR: sigtest must be either 0, 1, or 2')\n",
    "\n",
    "    return signif\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# CHISQUARE_INV  Inverse of chi-square cumulative distribution function (cdf).\n",
    "#\n",
    "#   X = chisquare_inv(P,V) returns the inverse of chi-square cdf with V\n",
    "#   degrees of freedom at fraction P.\n",
    "#   This means that P*100 percent of the distribution lies between 0 and X.\n",
    "#\n",
    "#   To check, the answer should satisfy:   P==gammainc(X/2,V/2)\n",
    "\n",
    "# Uses FMIN and CHISQUARE_SOLVE\n",
    "\n",
    "def chisquare_inv(P, V):\n",
    "\n",
    "    if (1 - P) < 1E-4:\n",
    "        print('P must be < 0.9999')\n",
    "\n",
    "    if P == 0.95 and V == 2:  # this is a no-brainer\n",
    "        X = 5.9915\n",
    "        return X\n",
    "\n",
    "    MINN = 0.01  # hopefully this is small enough\n",
    "    MAXX = 1  # actually starts at 10 (see while loop below)\n",
    "    X = 1\n",
    "    TOLERANCE = 1E-4  # this should be accurate enough\n",
    "\n",
    "    while (X + TOLERANCE) >= MAXX:  # should only need to loop thru once\n",
    "        MAXX = MAXX * 10.\n",
    "    # this calculates value for X, NORMALIZED by V\n",
    "        X = fminbound(chisquare_solve, MINN, MAXX, args=(P, V), xtol=TOLERANCE)\n",
    "        MINN = MAXX\n",
    "\n",
    "    X = X * V  # put back in the goofy V factor\n",
    "\n",
    "    return X  # end of code\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# CHISQUARE_SOLVE  Internal function used by CHISQUARE_INV\n",
    "    #\n",
    "    #   PDIFF=chisquare_solve(XGUESS,P,V)  Given XGUESS, a percentile P,\n",
    "    #   and degrees-of-freedom V, return the difference between\n",
    "    #   calculated percentile and P.\n",
    "\n",
    "    # Uses GAMMAINC\n",
    "    #\n",
    "    # Written January 1998 by C. Torrence\n",
    "\n",
    "    # extra factor of V is necessary because X is Normalized\n",
    "\n",
    "def chisquare_solve(XGUESS, P, V):\n",
    "\n",
    "    PGUESS = gammainc(V / 2, V * XGUESS / 2)  # incomplete Gamma function\n",
    "\n",
    "    PDIFF = np.abs(PGUESS - P)            # error in calculated P\n",
    "\n",
    "    TOL = 1E-4\n",
    "    if PGUESS >= 1 - TOL:  # if P is very close to 1 (i.e. a bad guess)\n",
    "        PDIFF = XGUESS   # then just assign some big number like XGUESS\n",
    "\n",
    "    return PDIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cwt(sst, cutoff_period=3):\n",
    "    sst = sst - np.mean(sst)\n",
    "    variance = np.std(sst, ddof=1) ** 2\n",
    "    if 0:\n",
    "        variance = 1.0\n",
    "        sst = sst / np.std(sst, ddof=1)\n",
    "    n = len(sst)\n",
    "    dt = 1\n",
    "    time = np.arange(len(sst)) * dt  # construct time array\n",
    "    xlim = ([0, 300])  # plotting range\n",
    "    pad = 1  # pad the time series with zeroes (recommended)\n",
    "    dj = 0.25  # this will do 4 sub-octaves per octave\n",
    "    s0 = 2 * dt  # this says start at a scale of 6 months\n",
    "    j1 = 7 / dj  # this says do 7 powers-of-two with dj sub-octaves each\n",
    "    lag1 = 0.72  # lag-1 autocorrelation for red noise background\n",
    "    mother = 'MORLET'\n",
    "\n",
    "    # Wavelet transform:\n",
    "    wave, period, scale, coi = wavelet(sst, dt, pad, dj, s0, j1, mother)\n",
    "    power = (np.abs(wave)) ** 2  # compute wavelet power spectrum\n",
    "    scale_avg = scale[:, np.newaxis].dot(np.ones(n)[np.newaxis, :])\n",
    "    power = power / scale_avg  # [Eqn(24)]\n",
    "    # REMOVE SMALL PERIODS and those under cone of influence\n",
    "    k = 0\n",
    "    while period[k] < cutoff_period:\n",
    "        k += 1\n",
    "    for i in range(k):\n",
    "        for j in range(len(power[0])):\n",
    "            power[i, j] = 0\n",
    "    for i in range(len(coi)):\n",
    "        j = 0\n",
    "        while period[j] <= coi[i]:\n",
    "            j += 1\n",
    "        while j < len(period):\n",
    "            power[j, i] = 0\n",
    "            j += 1\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gen'></a>\n",
    "___\n",
    "### Генерация 40000 синтетических сигналов\n",
    "\n",
    "[Вернуться наверх](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qpp(x, A_flare, l_flare):\n",
    "    phi = np.random.uniform(0, 2 * np.pi)\n",
    "    select_P = [10, 20, 30]\n",
    "    select_A_qpp = [0.1, 0.2, 0.3]\n",
    "    select_t_e = [1/10, 2/15, 1/5, 2/5]\n",
    "    P = l_flare / random.choice(select_P)\n",
    "    A_qpp = A_flare * random.choice(select_A_qpp)\n",
    "    t_e = l_flare * random.choice(select_t_e)\n",
    "    return A_qpp * np.exp(-x / t_e) * np.cos(2 * np.pi * x / P + phi)\n",
    "\n",
    "def add_noise(S):\n",
    "    N_white = S / random.choice(np.arange(3, 6))\n",
    "    r = np.random.uniform(0.81, 0.99)\n",
    "    N_red = S / max(1, 17 + np.random.normal(0, 1))\n",
    "    white_noise = np.random.normal(0, N_white, 300)\n",
    "    red_noise = np.zeros(300)\n",
    "    red_noise[0] = N_red / 2\n",
    "    for i in range(1, 300):\n",
    "        red_noise[i] = r * red_noise[i - 1] + \\\n",
    "        np.power((1 - r * r), 0.5) * white_noise[i]\n",
    "    noise = np.add(white_noise, red_noise)\n",
    "    return noise\n",
    "\n",
    "def polynomial(x, A=1):\n",
    "    x0 = 1\n",
    "    x1 = 1.941 + np.random.uniform(-0.008, 0.008)\n",
    "    x2 = -0.175 + np.random.uniform(-0.032, 0.032)\n",
    "    x3 = -2.246 + np.random.uniform(-0.039, 0.039)\n",
    "    x4 = -1.125 + np.random.uniform(-0.016, 0.016)\n",
    "    return A * (x0 + x1 * x + x2 * np.power(x, 2.) + x3 * np.power(x, 3.) + \\\n",
    "x4 * np.power(x, 4.))\n",
    "\n",
    "def exp1(x, A=1):\n",
    "    return A * 0.948 * np.exp(-0.965 * x)\n",
    "\n",
    "def exp2(x, A=1):\n",
    "    return A * 0.322 * np.exp(-0.290 * x)\n",
    "\n",
    "def generate_exponential(contains_qpp=False):\n",
    "    l_flare = np.random.uniform(100, 200)\n",
    "    t_peak = np.random.randint(30, 300 - l_flare)\n",
    "    A_flare = 10 + np.random.normal(0, 4)\n",
    "    if A_flare < 0.5:\n",
    "        A_flare = 0.5\n",
    "    peak_flare = l_flare * t_peak / 300\n",
    "    points1 = np.linspace(-1.0, 0.0, t_peak - 1, endpoint=False)\n",
    "    num_point2 = int((300 - t_peak + 1) * 1.6 / 7)\n",
    "    points2 = np.linspace(0.0, 1.6, num_point2 - 1, endpoint=False)\n",
    "    points3 = np.linspace(1.6, 7, 300 - t_peak + 1 - num_point2 + 1)\n",
    "    points_qpp = np.linspace(0.0, l_flare - peak_flare, 300 - t_peak + 1)\n",
    "    trend1 = polynomial(points1, A_flare)\n",
    "    trend2 = np.concatenate([exp1(points2, A_flare), exp2(points3, A_flare)])\n",
    "    if contains_qpp:\n",
    "        trend = np.concatenate([trend1, \\\n",
    "                                np.add(trend2, add_qpp(points_qpp, A_flare, l_flare))])\n",
    "    else:\n",
    "        trend = np.concatenate([trend1, trend2])\n",
    "    trend_with_noise = np.add(trend, add_noise(np.mean(trend)))\n",
    "    return trend_with_noise\n",
    "\n",
    "exponential_no_qpp = 0\n",
    "exponential_qpp = 0\n",
    "\n",
    "with open('one_type_signals.txt', 'w') as f:\n",
    "    for i in range(40000):\n",
    "        has_qpp = np.random.randint(2)\n",
    "        inf = np.zeros(2)\n",
    "        inf[0] = has_qpp\n",
    "        inf[1] = flare_type\n",
    "        if has_qpp:\n",
    "            flare = generate_exponential(True)\n",
    "            res = np.concatenate([inf, flare])\n",
    "            f.write(\"%s\\n\" % ' '.join(list(map(str, res))))\n",
    "            exponential_qpp += 1\n",
    "        else:\n",
    "            flare = generate_exponential(False)\n",
    "            res = np.concatenate([inf, flare])\n",
    "            f.write(\"%s\\n\" % ' '.join(list(map(str, res))))\n",
    "            exponential_no_qpp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='info'></a>\n",
    "___\n",
    "### Функция для получения информации об областях значимости\n",
    "\n",
    "[Вернуться наверх](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_smoothing(sign):\n",
    "    return sign - pd.Series(sign).rolling(10, min_periods=1).mean()\n",
    "\n",
    "def center_signal(sign):\n",
    "    m = np.mean(sign)\n",
    "    for j in range(0, len(sign)):\n",
    "        sign[j] -= m\n",
    "    return sign\n",
    "\n",
    "def change_signal(sign):\n",
    "    ind = np.argmax(sign)\n",
    "    sign2 = sign[ind:]\n",
    "    detrended = running_smoothing(sign2)\n",
    "    sign0 = np.hstack((np.zeros(300-len(detrended)), detrended))\n",
    "    return center_signal(sign0)\n",
    "\n",
    "def find_info(signal):\n",
    "    info = np.zeros(6)\n",
    "    \n",
    "    sst = signal\n",
    "    sst = change_signal(sst)\n",
    "    sst = sst - np.mean(sst)\n",
    "    variance = np.std(sst, ddof=1) ** 2\n",
    "    if 0:\n",
    "        variance = 1.0\n",
    "        sst = sst / np.std(sst, ddof=1)\n",
    "    n = len(sst)\n",
    "    dt = 1\n",
    "    time = np.arange(len(sst)) * dt\n",
    "    xlim = ([0, 300])\n",
    "    pad = 1\n",
    "    dj = 0.25\n",
    "    s0 = 2 * dt\n",
    "    j1 = 7 / dj\n",
    "    lag1 = 0.72\n",
    "    mother = 'MORLET'\n",
    "\n",
    "    wave, period, scale, coi = wavelet(sst, dt, pad, dj, s0, j1, mother)\n",
    "    power = (np.abs(wave)) ** 2\n",
    "    global_ws = (np.sum(power, axis=1) / n)\n",
    "\n",
    "    k = 0\n",
    "    while period[k] < 3:\n",
    "        k += 1\n",
    "    for i in range(3):\n",
    "        for j in range(len(power[0])):\n",
    "            power[i, j] = 0\n",
    "    for i in range(len(coi)):\n",
    "        j = 0\n",
    "        while period[j] <= coi[i]:\n",
    "            j += 1\n",
    "        while j < len(period):\n",
    "            power[j, i] = 0\n",
    "            j += 1\n",
    "    signif = wave_signif(([variance]), dt=dt, sigtest=0, scale=scale,\n",
    "        lag1=lag1, siglvl=0.95, mother=mother)\n",
    "    sig99 = signif[:, np.newaxis].dot(np.ones(n)[np.newaxis, :])\n",
    "    sig99 = power / sig99\n",
    "    \n",
    "    dof = n - scale\n",
    "    global_signif = wave_signif(variance, dt=dt, scale=scale, sigtest=1,\n",
    "        lag1=lag1, dof=dof, mother=mother)\n",
    "    CN = plt.contour(time, period, sig99, [-99, 1], colors='k')\n",
    "    figures = []\n",
    "    max_area = 0\n",
    "    if len(CN.allsegs) <= 1:\n",
    "        return info\n",
    "    for ii, seg in enumerate(CN.allsegs[1]):\n",
    "        pgon = Polygon(zip(seg[:,0], seg[:,1]))\n",
    "        bound = pgon.bounds # bounds: (minx, miny, maxx, maxy)\n",
    "        area = pgon.area\n",
    "        width = bound[2] - bound[0]\n",
    "        height = bound[3] - bound[1]\n",
    "        center_x = pgon.centroid.x\n",
    "        center_y = pgon.centroid.y\n",
    "        if area > max_area and center_y > 5 and center_y < 50:\n",
    "            max_area = area\n",
    "            info[0] = area\n",
    "            info[1] = width / center_y\n",
    "            info[2] = width\n",
    "            info[3] = height\n",
    "            info[4] = center_x\n",
    "            info[5] = center_y\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "%matplotlib agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получение информации об облястях значимости.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_file = 0\n",
    "with open('one_type_signals_info.txt', 'w') as f:\n",
    "    for i in range(len(X)):\n",
    "        has_qpp = np.zeros(1)\n",
    "        has_qpp[0] = y[i]\n",
    "        info = find_info(X[i])\n",
    "        res = np.concatenate([has_qpp, info])\n",
    "        f.write(\"%s\\n\" % ' '.join(list(map(str, res))))\n",
    "        len_file += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('one_type_signals_info.txt')\n",
    "y = data[:,0]\n",
    "X = data[:,2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "print(len(X_train), X_train[0].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='network1'></a>\n",
    "___\n",
    "### Перцептрон \n",
    "\n",
    "[Вернуться наверх](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 1s 832us/step - loss: 0.8236 - accuracy: 0.7446 - val_loss: 0.3933 - val_accuracy: 0.8319\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.4072 - accuracy: 0.8266 - val_loss: 0.4128 - val_accuracy: 0.8231\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 1s 737us/step - loss: 0.4015 - accuracy: 0.8282 - val_loss: 0.4213 - val_accuracy: 0.8250\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 1s 647us/step - loss: 0.4019 - accuracy: 0.8281 - val_loss: 0.4055 - val_accuracy: 0.8264\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.4037 - accuracy: 0.8258 - val_loss: 0.4095 - val_accuracy: 0.8229\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 1s 733us/step - loss: 0.3995 - accuracy: 0.8288 - val_loss: 0.4012 - val_accuracy: 0.8340\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.4014 - accuracy: 0.8296 - val_loss: 0.3925 - val_accuracy: 0.8279\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 1s 644us/step - loss: 0.4001 - accuracy: 0.8272 - val_loss: 0.3922 - val_accuracy: 0.8278\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.4074 - accuracy: 0.8221 - val_loss: 0.4334 - val_accuracy: 0.8200\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 1s 711us/step - loss: 0.3956 - accuracy: 0.8317 - val_loss: 0.3999 - val_accuracy: 0.8274\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 0.4008 - accuracy: 0.8269 - val_loss: 0.3902 - val_accuracy: 0.8315\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.3964 - accuracy: 0.8309 - val_loss: 0.3891 - val_accuracy: 0.8319\n",
      "Train loss: 0.3934752345085144, Train accuracy: 0.8302187323570251\n",
      "Test loss: 0.3890882730484009, Test accuracy: 0.8318750262260437\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=5, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "callback_early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', \\\n",
    "                                            patience=10, verbose=0, mode='auto')\n",
    "model.fit(X_train, y_train, epochs=12, batch_size=32, verbose=1, \\\n",
    "          validation_data=(X_test, y_test), callbacks=[callback_early_stopping])\n",
    "# evaluate the keras model\n",
    "train_score = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='network2'></a>\n",
    "___\n",
    "### Нейронная сеть с двумя входами\n",
    "\n",
    "[Вернуться наверх](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 5\n",
      "32000 300\n"
     ]
    }
   ],
   "source": [
    "data1 = np.loadtxt('one_type_signals_info.txt')\n",
    "y1 = data1[:,0]\n",
    "X1 = data1[:,2:]\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    X1, y1, test_size=0.2, random_state=42)\n",
    "print(len(X_train1), X_train1[0].size)\n",
    "\n",
    "data2 = np.loadtxt('one_type_signals.txt')\n",
    "y2 = data2[:,0]\n",
    "X2 = data2[:,2:]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2, y2, test_size=0.2, random_state=42)\n",
    "print(len(X_train2), X_train2[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_smoothing(sign):\n",
    "    detrended = sign - pd.Series(sign).rolling(10, min_periods=1).mean()\n",
    "    return detrended\n",
    "\n",
    "def center_signal(sign):\n",
    "    m = np.mean(sign)\n",
    "    for j in range(0, len(sign)):\n",
    "        sign[j] -= m\n",
    "    return sign\n",
    "\n",
    "def change_signal(sign):\n",
    "    ind = np.argmax(sign)\n",
    "    sign2 = sign[ind:]\n",
    "    detrended = running_smoothing(sign2)\n",
    "    sign0 = np.hstack((np.zeros(300-len(detrended)), detrended))\n",
    "    return center_signal(sign0)\n",
    "\n",
    "train_size2 = 32000\n",
    "test_size2 = 8000\n",
    "train_data_cwt = np.ndarray(shape=(train_size2, 29, 300))\n",
    "for i in range(0, train_size2):\n",
    "    signal = X_train2[i]\n",
    "    signal = change_signal(signal)\n",
    "    coeff_ = cwt(signal)\n",
    "    train_data_cwt[i, :, :] = coeff_\n",
    "\n",
    "test_data_cwt = np.ndarray(shape=(test_size2, 29, 300))\n",
    "for i in range(0,test_size2):\n",
    "    signal = X_test2[i]\n",
    "    signal = change_signal(signal)\n",
    "    coeff_ = cwt(signal)\n",
    "    test_data_cwt[i, :, :] = coeff_\n",
    "    \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (32000, 29, 300)\n",
      "32000 train samples\n",
      "8000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train2 = train_data_cwt\n",
    "x_test2 = test_data_cwt\n",
    "img_x = 29\n",
    "img_y = 300\n",
    "num_classes = 2\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "input_shape = (img_x, img_y)\n",
    "\n",
    "x_train2 = x_train2.astype('float32')\n",
    "x_test2 = x_test2.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train2.shape)\n",
    "print(x_train2.shape[0], 'train samples')\n",
    "print(x_test2.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 10s 9ms/step - loss: 0.4350 - accuracy: 0.7868 - val_loss: 0.2960 - val_accuracy: 0.8790\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.3051 - accuracy: 0.8759 - val_loss: 0.3015 - val_accuracy: 0.8846\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2911 - accuracy: 0.8893 - val_loss: 0.3038 - val_accuracy: 0.8829\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2784 - accuracy: 0.8904 - val_loss: 0.2741 - val_accuracy: 0.8874\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2663 - accuracy: 0.8962 - val_loss: 0.2878 - val_accuracy: 0.8865\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2570 - accuracy: 0.8979 - val_loss: 0.2765 - val_accuracy: 0.8852\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2523 - accuracy: 0.8993 - val_loss: 0.2885 - val_accuracy: 0.8821\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2552 - accuracy: 0.8966 - val_loss: 0.2818 - val_accuracy: 0.8876\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2583 - accuracy: 0.8965 - val_loss: 0.2788 - val_accuracy: 0.8859\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2502 - accuracy: 0.9020 - val_loss: 0.2947 - val_accuracy: 0.8821\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2478 - accuracy: 0.9047 - val_loss: 0.2851 - val_accuracy: 0.8813\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2458 - accuracy: 0.9058 - val_loss: 0.2815 - val_accuracy: 0.8845\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2282 - accuracy: 0.9111 - val_loss: 0.2826 - val_accuracy: 0.8871\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2181 - accuracy: 0.9135 - val_loss: 0.2962 - val_accuracy: 0.8839\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2384 - accuracy: 0.9084 - val_loss: 0.2885 - val_accuracy: 0.8827\n",
      "Train loss: 0.2141030728816986, Train accuracy: 0.9157500267028809\n",
      "Test loss: 0.2885476052761078, Test accuracy: 0.8827499747276306\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv1D(32, kernel_size=5, strides=1, activation='relu', input_shape=input_shape)) \n",
    "cnn.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "cnn.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(1024, activation='relu'))\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(12, input_dim=5, activation='relu'))\n",
    "mlp.add(Dense(8, activation='relu'))\n",
    "mlp.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train the model\n",
    "model.fit(x=[X_train1, x_train2], y=y_train1,\\\n",
    "          validation_data=([X_test1, x_test2], y_test1),\\\n",
    "          epochs=15, batch_size=32, callbacks=[history])\n",
    "\n",
    "train_score = model.evaluate(x=[X_train1, x_train2], y=y_train1, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate([X_test1, x_test2], y_test1, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='network3'></a>\n",
    "___\n",
    "### Сверточная нейронная сеть\n",
    "\n",
    "[Вернуться наверх](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 300\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('one_type_signals.txt')\n",
    "y = data[:,0]\n",
    "X = data[:,2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(len(X_train), X_train[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_smoothing(sign):\n",
    "    detrended = sign - pd.Series(sign).rolling(10, min_periods=0, center=True).mean()\n",
    "    return detrended\n",
    "\n",
    "def center_signal(sign):\n",
    "    m = np.mean(sign)\n",
    "    for j in range(0, len(sign)):\n",
    "        sign[j] -= m\n",
    "    return sign\n",
    "\n",
    "def change_signal(sign):\n",
    "    ind = np.argmax(sign)\n",
    "    sign2 = sign[ind:]\n",
    "    detrended = running_smoothing(sign2)\n",
    "    sign0 = np.hstack((np.zeros(300-len(detrended)), detrended))\n",
    "    return center_signal(sign0)\n",
    "\n",
    "train_size = 32000\n",
    "test_size = 8000\n",
    "\n",
    "train_data_cwt = np.ndarray(shape=(train_size, 29, 300))\n",
    "for i in range(0, train_size):\n",
    "    signal = X_train[i]\n",
    "    signal = change_signal(signal)\n",
    "    coeff_ = cwt(signal)\n",
    "    train_data_cwt[i, :, :] = coeff_\n",
    "\n",
    "test_data_cwt = np.ndarray(shape=(test_size, 29, 300))\n",
    "for i in range(0,test_size):\n",
    "    signal = X_test[i]\n",
    "    signal = change_signal(signal)\n",
    "    coeff_ = cwt(signal)\n",
    "    test_data_cwt[i, :, :] = coeff_\n",
    "    \n",
    "history20 = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (32000, 29, 300)\n",
      "32000 train samples\n",
      "8000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data_cwt\n",
    "x_test = test_data_cwt\n",
    "img_x = 29\n",
    "img_y = 300\n",
    "num_classes = 2\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "input_shape = (img_x, img_y)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 29s 10ms/step - loss: 0.3955 - accuracy: 0.8195 - val_loss: 0.3024 - val_accuracy: 0.8698\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2969 - accuracy: 0.8768 - val_loss: 0.2915 - val_accuracy: 0.8783\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2861 - accuracy: 0.8804 - val_loss: 0.3022 - val_accuracy: 0.8664\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2768 - accuracy: 0.8854 - val_loss: 0.3013 - val_accuracy: 0.8715\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2703 - accuracy: 0.8902 - val_loss: 0.2894 - val_accuracy: 0.8766\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2611 - accuracy: 0.8907 - val_loss: 0.2840 - val_accuracy: 0.8827\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2551 - accuracy: 0.8938 - val_loss: 0.2978 - val_accuracy: 0.8779\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2540 - accuracy: 0.8941 - val_loss: 0.2984 - val_accuracy: 0.8769\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2406 - accuracy: 0.9004 - val_loss: 0.3052 - val_accuracy: 0.8783\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2265 - accuracy: 0.9085 - val_loss: 0.3143 - val_accuracy: 0.8729\n",
      "Train loss: 0.2230750024318695, Train accuracy: 0.9085624814033508\n",
      "Test loss: 0.314344197511673, Test accuracy: 0.8728749752044678\n"
     ]
    }
   ],
   "source": [
    "model20 = Sequential()\n",
    "model20.add(Conv1D(32, kernel_size=5, strides=1, activation='relu', input_shape=input_shape)) \n",
    "model20.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "model20.add(Conv1D(64, 5, activation='relu'))\n",
    "model20.add(MaxPooling1D(pool_size=2))\n",
    "model20.add(Flatten())\n",
    "model20.add(Dense(1024, activation='relu'))\n",
    "model20.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model20.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=\"adam\", \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model20.fit(x_train, y_train, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=1, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[history20])\n",
    "\n",
    "train_score = model20.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model20.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
